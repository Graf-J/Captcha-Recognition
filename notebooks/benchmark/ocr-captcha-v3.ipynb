{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6f0a7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VisionEncoderDecoderModel, TrOCRProcessor\n",
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(\"../..\")))\n",
    "from src.datasets.kaggledataset import KaggleDataset  # noqa: E402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a58b4428",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "058af04806de49d38903b656ab4b7026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/364 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `ViTImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1de7fc5cd1ab424b8511614083e0c7b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d13de1310ee948179519d536af8a68cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a53167664224c069926eda268cf4bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ef50dc4c19e4470b35a64511e3577b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/957 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1d4ebb4f09945008fb03a5475fde99d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e1b3cd01ee4405d9ce5351208b4a51e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/480 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ed0d6290dde4cf3bde87c4155f83bd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/288 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model and processor\n",
    "processor = TrOCRProcessor.from_pretrained(\"anuashok/ocr-captcha-v3\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"anuashok/ocr-captcha-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14917702",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = KaggleDataset(os.environ[\"KAGGLE_ROOT_DIR\"], preload=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d556c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa1354ffd9bc4119abca99c679ca3332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running OCR inference:   0%|          | 0/3534 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ----------------------\n",
    "# Configuration\n",
    "# ----------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "batch_size = 32  # Adjust depending on GPU memory\n",
    "\n",
    "# Custom collate to keep images and labels separate\n",
    "def collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    return list(images), list(labels)\n",
    "\n",
    "# DataLoader (parallel loading)\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=8,  # CPU threads for preprocessing\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# ----------------------\n",
    "# Inference Loop\n",
    "# ----------------------\n",
    "max_batches = 5  # Number of batches you want to run\n",
    "results = []\n",
    "\n",
    "for batch_idx, (images, labels) in enumerate(tqdm(loader, desc=\"Running OCR inference\")):\n",
    "    if batch_idx >= max_batches:\n",
    "        break  # Stop after max_batches\n",
    "\n",
    "    # ------------------------\n",
    "    # 1️⃣ Preprocess images properly\n",
    "    # ------------------------\n",
    "    processed_images = []\n",
    "    for img in images:\n",
    "        # 1. Convert to RGBA\n",
    "        img_rgba = img.convert(\"RGBA\")\n",
    "        # 2. White background\n",
    "        background = Image.new(\"RGBA\", img_rgba.size, (255, 255, 255, 255))\n",
    "        # 3. Alpha composite\n",
    "        img_composite = Image.alpha_composite(background, img_rgba)\n",
    "        # 4. Convert back to RGB\n",
    "        img_rgb = img_composite.convert(\"RGB\")\n",
    "        processed_images.append(img_rgb)\n",
    "\n",
    "    # 2️⃣ Generate model inputs\n",
    "    pixel_values = processor(processed_images, return_tensors=\"pt\").pixel_values.to(device)\n",
    "\n",
    "    # 3️⃣ Generate predictions\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(pixel_values)\n",
    "\n",
    "    preds = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    # 4️⃣ Convert labels to strings\n",
    "    actuals = [\"\".join(dataset.idx_to_char[i.item()] for i in t) for t in labels]\n",
    "\n",
    "    # 5️⃣ Store results\n",
    "    for i, (a, p) in enumerate(zip(actuals, preds)):\n",
    "        results.append({\n",
    "            \"idx\": batch_idx * batch_size + i,\n",
    "            \"actual\": a,\n",
    "            \"pred\": p\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55595fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   idx actual    pred\n",
      "0    0  x8jy8   x8jy8\n",
      "1    1  1VTNw   NVTNW\n",
      "2    2  N324n  TN324n\n",
      "3    3  1ZlCO  JZ100-\n",
      "4    4  OvTL4  TOTAL4\n"
     ]
    }
   ],
   "source": [
    "df_results = pd.DataFrame(results)\n",
    "print(df_results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63c85ccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(9.375)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_results[\"actual\"] == df_results[\"pred\"]).sum() / len(df_results) * 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "captcha-recognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
